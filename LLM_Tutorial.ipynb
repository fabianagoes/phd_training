{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eee5b7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# Tutorial (Colab) â€” Fine-tuning step by step (English, no custom wrappers)\n",
    "# - No pair-task support (only single sequence classification: text,label)\n",
    "# - Three modes: full fine-tuning, head-only, or LoRA\n",
    "# - Expects train.csv, dev.csv and test.csv with format: text,label\n",
    "# =============================================================\n",
    "\n",
    "# If running on Google Colab, uncomment:\n",
    "!pip install -q transformers peft datasets accelerate scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b361322",
   "metadata": {},
   "source": [
    "# 1) Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10ea59f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "!git clone https://github.com/fabianagoes/bc2_tutorial8.git\n",
    "%cd bc2_tutorial8\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, csv, json, logging, random, warnings\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, precision_score, recall_score\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\".*torch.cuda.amp.autocast.*\")\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d525e",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038be2e5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# zhihan1996/DNABERT-2-117M\n",
    "# InstaDeepAI/nucleotide-transformer-500m-human-ref\n",
    "model_name_or_path = \"InstaDeepAI/nucleotide-transformer-500m-human-ref\"  # example: \"bert-base-uncased\", \"InstaDeepAI/nucleotide-transformer-v2-50m-1000g\"\n",
    "\n",
    "# Training strategy (choose ONE)\n",
    "use_lora = False          # True for LoRA\n",
    "train_head_only = False   # True to train only the classification head\n",
    "train_head_layer10 = True # True to train only layer 10 and the classification head\n",
    "# If both are False => full fine-tuning\n",
    "\n",
    "# LoRA parameters (if use_lora=True)\n",
    "lora_r = 8\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.05\n",
    "lora_target_modules = \"query,value\"  # Adjust for your model, e.g. \"q_proj,v_proj\"\n",
    "\n",
    "# Data - Must contain train.csv, dev.csv, test.csv\n",
    "data_path = \"/content/bc2_tutorial8/datasets/GUE/prom/prom_300_notata\"\n",
    "\n",
    "# Training args\n",
    "run_name = \"run\"\n",
    "output_dir = \"/content/bc2_tutorial8/output\"\n",
    "model_max_length = 512\n",
    "per_device_train_batch_size = 8\n",
    "per_device_eval_batch_size = 16\n",
    "num_train_epochs = 1\n",
    "fp16 = False\n",
    "save_strategy = \"epoch\"\n",
    "evaluation_strategy = \"epoch\"\n",
    "eval_steps = 100\n",
    "warmup_steps = 50\n",
    "weight_decay = 0.1\n",
    "learning_rate = 1e-4\n",
    "save_total_limit = None\n",
    "load_best_model_at_end = True\n",
    "seed = 42\n",
    "\n",
    "# Save model and results\n",
    "save_model = True\n",
    "save_results = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7c13ec",
   "metadata": {},
   "source": [
    "# 3) Seed and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae476a95",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbaf62e",
   "metadata": {},
   "source": [
    "# 4) Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac0091d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nLoading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    model_max_length=model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "if \"InstaDeepAI\" in model_name_or_path and tokenizer.eos_token is None and tokenizer.pad_token is not None:\n",
    "    tokenizer.eos_token = tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d1d9f7",
   "metadata": {},
   "source": [
    "# 5) Read CSVs (single sequence classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0579fd16",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nReading data...\")\n",
    "print(\"Task: single sequence classification (text,label)\")\n",
    "\n",
    "def stratified_sample(df, frac, label_col=\"label\", random_state=42):\n",
    "    return (\n",
    "        df.groupby(label_col, group_keys=False)\n",
    "          .apply(lambda x: x.sample(frac=frac, random_state=random_state))\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n",
    "dev_df   = pd.read_csv(os.path.join(data_path, \"dev.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(data_path, \"test.csv\"))\n",
    "\n",
    "train_df = stratified_sample(train_df, 0.01)\n",
    "dev_df   = stratified_sample(dev_df, 0.08)\n",
    "test_df  = stratified_sample(test_df, 0.08)\n",
    "\n",
    "print(\"\\nAfter stratified sampling:\")\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Dev size:\", len(dev_df))\n",
    "print(\"Test size:\", len(test_df))\n",
    "print(\"\\nClass distribution in train:\")\n",
    "print(train_df[\"label\"].value_counts())\n",
    "\n",
    "train_texts, train_labels = train_df[\"sequence\"].tolist(), train_df[\"label\"].astype(int).tolist()\n",
    "dev_texts, dev_labels     = dev_df[\"sequence\"].tolist(), dev_df[\"label\"].astype(int).tolist()\n",
    "test_texts, test_labels   = test_df[\"sequence\"].tolist(), test_df[\"label\"].astype(int).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d1b60f",
   "metadata": {},
   "source": [
    "# 6) Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d24eb29",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nTokenizing train/dev/test...\")\n",
    "enc_train = tokenizer(train_texts, truncation=True, padding=False, max_length=model_max_length)\n",
    "enc_dev = tokenizer(dev_texts, truncation=True, padding=False, max_length=model_max_length)\n",
    "enc_test = tokenizer(test_texts, truncation=True, padding=False, max_length=model_max_length)\n",
    "\n",
    "# take the first example\n",
    "print(\"Original text:\")\n",
    "print(train_texts[0])\n",
    "\n",
    "print(\"\\nToken IDs:\")\n",
    "print(enc_train[\"input_ids\"][0])\n",
    "\n",
    "print(\"\\nDecoded tokens:\")\n",
    "print(tokenizer.convert_ids_to_tokens(enc_train[\"input_ids\"][0]))\n",
    "\n",
    "print(\"\\nReconstructed sequence:\")\n",
    "print(tokenizer.decode(enc_train[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd641b4",
   "metadata": {},
   "source": [
    "# 7) Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a073e97",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_dict({\n",
    "    \"input_ids\": enc_train[\"input_ids\"],\n",
    "    \"attention_mask\": enc_train[\"attention_mask\"],\n",
    "    \"labels\": train_labels,\n",
    "})\n",
    "\n",
    "dev_ds = Dataset.from_dict({\n",
    "    \"input_ids\": enc_dev[\"input_ids\"],\n",
    "    \"attention_mask\": enc_dev[\"attention_mask\"],\n",
    "    \"labels\": dev_labels,\n",
    "})\n",
    "\n",
    "test_ds = Dataset.from_dict({\n",
    "    \"input_ids\": enc_test[\"input_ids\"],\n",
    "    \"attention_mask\": enc_test[\"attention_mask\"],\n",
    "    \"labels\": test_labels,\n",
    "})\n",
    "\n",
    "num_labels = len(set(train_labels + dev_labels + test_labels))\n",
    "print(\"num_labels:\", num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839e0edd",
   "metadata": {},
   "source": [
    "# 8) Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d528dc9c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nLoading model...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Num epochs:\", num_train_epochs)\n",
    "print(\"Save strategy:\", save_strategy)\n",
    "print(\"Load best model at end:\", load_best_model_at_end)\n",
    "print(\"Train head only:\", train_head_only)\n",
    "print(\"Use LoRA:\", use_lora)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec3fc34",
   "metadata": {},
   "source": [
    "# 9) Training strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd14646a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if train_head_only and use_lora:\n",
    "    raise ValueError(\"Choose only ONE strategy: head-only or LoRA (or both False for full FT)\")\n",
    "\n",
    "if train_head_only:\n",
    "    print(\"\\n[Head-only] Freezing backbone and leaving only classifier trainable...\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(\"classifier\"): # Trainable classification head\n",
    "            param.requires_grad = True\n",
    "        else: # Freeze everything else\n",
    "            param.requires_grad = False\n",
    "\n",
    "elif train_head_layer10:\n",
    "    print(\"\\n[Head and layer 10] Freezing backbone and leaving only classifier and layer 10 trainable...\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"layer.10.\" in name or name.startswith(\"classifier\"):\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "elif use_lora:\n",
    "    print(\"\\n[LoRA] Applying efficient adaptation...\")\n",
    "    lconf = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=[s.strip() for s in lora_target_modules.split(\",\") if s.strip()],\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\",\n",
    "        inference_mode=False,\n",
    "    )\n",
    "    model = get_peft_model(model, lconf)\n",
    "    model.print_trainable_parameters()\n",
    "else:\n",
    "    print(\"\\n[Full fine-tuning] All parameters will be updated.\")\n",
    "\n",
    "# We can check whether the model was correctly updated\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"\\nParameter: {name} ----- {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2a7e0b",
   "metadata": {},
   "source": [
    "# 10) Data collator and TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d88593f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    run_name=run_name,\n",
    "    optim=\"adamw_torch\",\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    fp16=fp16,\n",
    "    save_strategy=save_strategy,\n",
    "    eval_strategy=evaluation_strategy,\n",
    "    eval_steps=eval_steps if evaluation_strategy == \"steps\" else None,\n",
    "    warmup_steps=warmup_steps,\n",
    "    weight_decay=weight_decay,\n",
    "    learning_rate=learning_rate,\n",
    "    save_total_limit=save_total_limit,\n",
    "    load_best_model_at_end=load_best_model_at_end,\n",
    "    dataloader_pin_memory=False,\n",
    "    seed=seed,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec59c9a",
   "metadata": {},
   "source": [
    "# 11) Metrics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991cfad1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    preds = np.argmax(predictions, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"matthews_correlation\": matthews_corrcoef(labels, preds),\n",
    "        \"precision\": precision_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"recall\": recall_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c85aa13",
   "metadata": {},
   "source": [
    "# 12) Trainer and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f1f3fd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=dev_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b1d03c",
   "metadata": {},
   "source": [
    "# 13) Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edb7a5c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating on test set...\")\n",
    "results = trainer.evaluate(eval_dataset=test_ds)\n",
    "print(\"\\nMetrics (test):\\n\", json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f499cc",
   "metadata": {},
   "source": [
    "# 14) Save results and model (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9cb376",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    results_path = os.path.join(output_dir, \"results\", run_name)\n",
    "    os.makedirs(results_path, exist_ok=True)\n",
    "    with open(os.path.join(results_path, \"eval_results.json\"), \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"\\nResults saved at: {os.path.join(results_path, 'eval_results.json')}\")\n",
    "\n",
    "if save_model:\n",
    "    final_model_dir = os.path.join(output_dir, \"final_model\")\n",
    "    trainer.save_model(final_model_dir)\n",
    "    print(f\"Model saved at: {final_model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ccf5de",
   "metadata": {},
   "source": [
    "# 15) Compare with MLP on frozen embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2951e003",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"\\nExtracting embeddings with frozen pretrained model...\")\n",
    "\n",
    "# Load model without classification head\n",
    "base_model = AutoModel.from_pretrained(model_name_or_path, trust_remote_code=True).to(device)\n",
    "base_model.eval()\n",
    "\n",
    "def extract_embeddings(texts, batch_size=16):\n",
    "    all_embs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(batch, truncation=True, padding=True, max_length=model_max_length, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = base_model(**enc)\n",
    "            # Take CLS token embedding (first token) as representation\n",
    "            emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        all_embs.append(emb)\n",
    "    return np.vstack(all_embs)\n",
    "\n",
    "X_train = extract_embeddings(train_texts)\n",
    "X_dev   = extract_embeddings(dev_texts)\n",
    "X_test  = extract_embeddings(test_texts)\n",
    "print(\"Embeddings shape:\", X_train.shape)\n",
    "\n",
    "# Train MLP\n",
    "print(\"\\nTraining MLP classifier on frozen embeddings...\")\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(256,), max_iter=20, random_state=seed)\n",
    "mlp.fit(X_train, train_labels)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nMLP performance on test set:\")\n",
    "mlp_preds = mlp.predict(X_test)\n",
    "print(classification_report(test_labels, mlp_preds, digits=4))\n",
    "\n",
    "# Compare with fine-tuned model\n",
    "print(\"\\nClassification report (fine-tuned model):\")\n",
    "predictions = trainer.predict(test_ds)\n",
    "y_true = predictions.label_ids\n",
    "y_pred = np.argmax(predictions.predictions, axis=-1)\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
